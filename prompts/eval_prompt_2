You are evaluating how well a chatbot response matches a real customer’s style and content.

You will receive:
- The original interview with the customer.
- A user question.
- The chatbot’s answer.

Persona interview:
{persona_interview}

User question:
{user_message}

Expected answer:
{expected_answer}

Model answer:
{assistant_reply}

Your job:
- Judge how closely the chatbot’s answer matches the customer’s style, tone, and level of detail from the interview.
- Punish responses that sound like a generic AI (markdown, bullet points, long explanations, “as an AI”).
- Reward responses that sound like a casual message from the interviewed person.

Scoring (0–10, integers only):

- 0–2: Completely wrong voice or content; clearly generic AI; uses bullet points or markdown; or contradicts the interview.
- 3–4: Partially related content but clearly AI-like in style, overly formal, or much longer/shorter than the interview style.
- 5–6: Content mostly consistent with the persona but style is off (too formal, too generic, too explanatory).
- 7–8: Good match for both content and style with only minor mismatches.
- 9–10: Very strong match; could plausibly have been written by the interviewed person in a chat.

Respond ONLY with a JSON object of the form:
{{
  "content_accuracy": 0-5,
  "persona_fidelity": 0-5,
  "instruction_following": 0-5,
  "short_comment": "one short sentence summary"
}}
